{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## _*AI Assignment #3*_\n",
        "\n",
        "Name|ID\n",
        "-|-------------\n",
        "Louai Nasr Zahran|19016198\n",
        "ÙAbdelrahman Ahmed Bahaa|19015882\n",
        "Gamal Abdelhamed Nasef|19015550\n",
        "Mohamed Ayman Saeed|19016250\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sp_NVfgyn5PW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIQ5UtdWq8mo"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "import math\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "\n",
        "# The reward values for each cell\n",
        "# The upper left cell is initialized to a random value and edited afterwards \n",
        "# to take each of the values [100, -3, 0, 3] as required\n",
        "reward = [[0, -1, 10],\n",
        "        [-1, -1, -1],\n",
        "        [-1, -1, -1]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of value iteration using the bellman equations\n",
        "# r -> value of upper left corner\n",
        "# theta -> cell difference on which we terminate the algorithm\n",
        "# discount -> gamma in bellman equations\n",
        "# returns: optimal policy with obtained values in last iteration\n",
        "def value_iteration(r: float, theta: float, discount: float):\n",
        "\n",
        "  # Initialize policy and values lists\n",
        "  policy = [[\"\", \"\", \"\"] for i in range(3)]\n",
        "  value = [[0, 0, 0] for i in range(3)]\n",
        "\n",
        "  # Update R\n",
        "  reward[0][0] = r\n",
        "\n",
        "  # delta is the max of the actual measured differences between the value of\n",
        "  # the current iteration and the previous iteration\n",
        "  delta = math.inf\n",
        "\n",
        "  # loop until delta becomes smaller than desired theta\n",
        "  while delta > theta:\n",
        "\n",
        "    # prepare for new iteration\n",
        "    previous_value = value\n",
        "    value = [[0, 0, 0] for i in range(3)]\n",
        "    delta = 0\n",
        "\n",
        "    # loop over all states\n",
        "    for i in range(3):\n",
        "      for j in range(3):\n",
        "\n",
        "        # ignore terminal states\n",
        "        if i == 0 and (j == 0 or j == 2):\n",
        "          policy[i][j] = \"Exit\"\n",
        "          continue\n",
        "\n",
        "        # loop over all possible actions\n",
        "        for action in [\"L\", \"R\", \"U\", \"D\"]:\n",
        "          val = 0\n",
        "          all_actions = get_possible_actions(action)\n",
        "\n",
        "          # loop over all possible outcomes for desired action\n",
        "          for possible_action in all_actions:\n",
        "            if possible_action == action:\n",
        "              prob = 0.8\n",
        "            else:\n",
        "              prob = 0.1\n",
        "            \n",
        "            next_i, next_j = get_new_state(i, j, possible_action)\n",
        "\n",
        "            # apply bellman equation\n",
        "            val += prob * (reward[next_i][next_j] + discount * previous_value[next_i][next_j])\n",
        "\n",
        "          # update new iteration's delta, values, and policy\n",
        "          delta = max(delta, val - previous_value[i][j])\n",
        "\n",
        "          if val > value[i][j]:\n",
        "            value[i][j] = val\n",
        "            policy[i][j] = action\n",
        "\n",
        "  # fix the value of terminal states (as they initialized to 0)\n",
        "  value[0][0] = r\n",
        "  value[0][2] = 10\n",
        "  return value, policy"
      ],
      "metadata": {
        "id": "6fE3FeA_sJct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(r: float, theta: float, discount: float):\n",
        "  # Initialization (same as value iteration, but assign random initial policy)\n",
        "  randomArr= np.random.random((3,3))\n",
        "  policy = [[\"\", \"\", \"\"] for i in range(3)]\n",
        "  \n",
        "  for i in range(3):\n",
        "    for j in range(3):\n",
        "      policy[i][j] = getRandomState(randomArr[i][j])\n",
        "\n",
        "  policy[0][0] = \"Exit\"\n",
        "  policy[0][2] = \"Exit\"\n",
        "\n",
        "  value = [[0, 0, 0] for i in range(3)]\n",
        "  reward[0][0] = r\n",
        "  delta = math.inf\n",
        "  policy_changed = True\n",
        "\n",
        "  # loop until the policy doesn't change for two iterations\n",
        "  while policy_changed:\n",
        "    policy_changed = False\n",
        "    \n",
        "    # Policy Evaluation\n",
        "    while delta > theta:\n",
        "      previous_value = value\n",
        "      delta = 0\n",
        "\n",
        "      for i in range(3):\n",
        "        for j in range(3):\n",
        "          if i == 0 and (j == 0 or j == 2):\n",
        "            continue\n",
        "\n",
        "          action = policy[i][j]\n",
        "          val = 0\n",
        "          all_actions = get_possible_actions(action)\n",
        "          for possible_action in all_actions:\n",
        "            if possible_action == action:\n",
        "              prob = 0.8\n",
        "            else:\n",
        "              prob = 0.1\n",
        "            \n",
        "            next_i, next_j = get_new_state(i, j, possible_action)\n",
        "            val += prob * (reward[next_i][next_j] + discount * previous_value[next_i][next_j])\n",
        "\n",
        "          delta = max(delta, val - previous_value[i][j])\n",
        "\n",
        "          value[i][j] = val\n",
        "\n",
        "\n",
        "    # Policy Improvement\n",
        "    for i in range(3):\n",
        "      for j in range(3):\n",
        "        if i == 0 and (j == 0 or j == 2):\n",
        "          continue\n",
        "\n",
        "        for action in [\"L\", \"R\", \"U\", \"D\"]:\n",
        "          val = 0\n",
        "          all_actions = get_possible_actions(action)\n",
        "          for possible_action in all_actions:\n",
        "            if possible_action == action:\n",
        "              prob = 0.8\n",
        "            else:\n",
        "              prob = 0.1\n",
        "            \n",
        "            next_i, next_j = get_new_state(i, j, possible_action)\n",
        "            val += prob * (reward[next_i][next_j] + discount * previous_value[next_i][next_j])\n",
        "\n",
        "          if val > value[i][j]:\n",
        "            value[i][j] = val\n",
        "            if action != policy[i][j]:\n",
        "              policy_changed = True\n",
        "              policy[i][j] = action\n",
        "\n",
        "  value[0][0] = r\n",
        "  value[0][2] = 10\n",
        "\n",
        "\n",
        "  return value, policy"
      ],
      "metadata": {
        "id": "_Po1DqeD7yTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getRandomState(i):\n",
        "  if i<0.25:\n",
        "    return \"U\"\n",
        "  elif i<0.5:\n",
        "    return \"R\"\n",
        "  elif i<0.75:\n",
        "    return \"D\"\n",
        "  else:\n",
        "    return \"L\""
      ],
      "metadata": {
        "id": "s3Jf1qqKQGHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_state(i: int, j: int, action: str):\n",
        "  if action == \"U\":\n",
        "    new_i = i - 1\n",
        "    new_j = j\n",
        "  if action == \"D\":\n",
        "    new_i = i + 1\n",
        "    new_j = j\n",
        "  if action == \"L\":\n",
        "    new_i = i\n",
        "    new_j = j - 1\n",
        "  if action == \"R\":\n",
        "    new_i = i\n",
        "    new_j = j + 1\n",
        "  \n",
        "  if new_i == -1 or new_i == 3 or new_j == -1 or new_j == 3:\n",
        "    return i, j\n",
        "  return new_i, new_j"
      ],
      "metadata": {
        "id": "iCBUGgPDuspS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_possible_actions(action: str):\n",
        "  if action == \"U\":\n",
        "    return [\"U\", \"R\", \"L\"]\n",
        "  if action == \"L\":\n",
        "    return [\"L\", \"U\", \"D\"]\n",
        "  if action == \"R\":\n",
        "    return [\"R\", \"U\", \"D\"]\n",
        "  if action == \"D\":\n",
        "    return [\"D\", \"L\", \"R\"]\n",
        "  \n",
        "  return []"
      ],
      "metadata": {
        "id": "nkeY5m9gwPH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = time.time()\n",
        "rArr=[100,3,0,-3]\n",
        "for r in rArr:\n",
        "  print(\"===================================================\")\n",
        "  print(\"r = \"+str(r))\n",
        "  value, policy = value_iteration(r, 0.01, 0.99)\n",
        "  print(\"Time taken value: \" + str(time.time() - t))\n",
        "  print(tabulate(value))\n",
        "  print(tabulate(policy))\n",
        "  print()\n",
        "\n",
        "  t = time.time()\n",
        "  value, policy = policy_iteration(r, 0.01, 0.99)\n",
        "  print(\"Time taken policy: \" + str(time.time() - t))\n",
        "  print(tabulate(value))\n",
        "  print(tabulate(policy))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIABMCW5ytzk",
        "outputId": "a88d33be-4b4a-49c8-d6e8-344029b69f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================================\n",
            "r = 100\n",
            "Time taken value: 0.0057370662689208984\n",
            "--------  -------  -------\n",
            "100       99.1954  10\n",
            " 99.1954  96.7181  90.0975\n",
            " 96.4459  94.295   91.6752\n",
            "--------  -------  -------\n",
            "----  -  ----\n",
            "Exit  L  Exit\n",
            "U     L  D\n",
            "U     L  L\n",
            "----  -  ----\n",
            "\n",
            "Time taken policy: 0.0025599002838134766\n",
            "--------  -------  -------\n",
            "100       99.0945  10\n",
            " 99.0939  96.51    88.1159\n",
            " 96.1524  93.9246  90.9992\n",
            "--------  -------  -------\n",
            "----  -  ----\n",
            "Exit  L  Exit\n",
            "U     L  D\n",
            "U     L  L\n",
            "----  -  ----\n",
            "\n",
            "===================================================\n",
            "r = 3\n",
            "Time taken value: 0.011793851852416992\n",
            "-------  -------  --------\n",
            "3        9.55728  10\n",
            "6.44537  8.19421   9.55728\n",
            "5.62534  6.86024   8.04434\n",
            "-------  -------  --------\n",
            "----  -  ----\n",
            "Exit  R  Exit\n",
            "R     R  U\n",
            "R     R  U\n",
            "----  -  ----\n",
            "\n",
            "Time taken policy: 0.0008528232574462891\n",
            "-------  -------  --------\n",
            "3        9.54739  10\n",
            "6.35029  8.17335   9.55431\n",
            "5.47879  6.82343   8.03595\n",
            "-------  -------  --------\n",
            "----  -  ----\n",
            "Exit  R  Exit\n",
            "R     R  U\n",
            "R     R  U\n",
            "----  -  ----\n",
            "\n",
            "===================================================\n",
            "r = 0\n",
            "Time taken value: 0.0067713260650634766\n",
            "-------  -------  --------\n",
            "0        9.55726  10\n",
            "6.14183  8.19412   9.55726\n",
            "5.59148  6.86001   8.04424\n",
            "-------  -------  --------\n",
            "----  -  ----\n",
            "Exit  R  Exit\n",
            "R     R  U\n",
            "R     R  U\n",
            "----  -  ----\n",
            "\n",
            "Time taken policy: 0.0014722347259521484\n",
            "-------  -------  --------\n",
            "0        9.54431  10\n",
            "6.01675  8.16698   9.55337\n",
            "5.39977  6.81209   8.03319\n",
            "-------  -------  --------\n",
            "----  -  ----\n",
            "Exit  R  Exit\n",
            "R     R  U\n",
            "R     R  U\n",
            "----  -  ----\n",
            "\n",
            "===================================================\n",
            "r = -3\n",
            "Time taken value: 0.00816035270690918\n",
            "--------  -------  --------\n",
            "-3        9.55726  10\n",
            " 5.83853  8.19412   9.55726\n",
            " 5.55815  6.86001   8.04424\n",
            "--------  -------  --------\n",
            "----  -  ----\n",
            "Exit  R  Exit\n",
            "R     R  U\n",
            "R     R  U\n",
            "----  -  ----\n",
            "\n",
            "Time taken policy: 0.0009524822235107422\n",
            "--------  -------  --------\n",
            "-3        9.54541  10\n",
            " 5.72273  8.16932   9.55371\n",
            " 5.38173  6.81638   8.03423\n",
            "--------  -------  --------\n",
            "----  -  ----\n",
            "Exit  R  Exit\n",
            "R     R  U\n",
            "R     R  U\n",
            "----  -  ----\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**At r = 100**\n",
        "\n",
        "*   The agent will likely go towards r as the reward of other states is just -1 and it can take 100 when reaching r.\n",
        "*   States around the final state that have a reward of *10 points*  will move to the opposite direction to avoid any possibility of going into it.\n",
        "\n",
        "\n",
        "**At r = 3, 0, -3**\n",
        "\n",
        "*   The agent will likely go towards final state that have 10 reward as the reward of other cells is just -1 and it can reach the state that has a 10 points reward from any state using a maximum of 4 moves.\n",
        "*   The arrow in the state below r is pointing to right going to state 10 as fast as possible. avoiding to make a detour if it go down as it will cost more than going right.\n",
        "*   Note for r = 3 we can calculate if we go to r directly from the farthest point from 10 so we can see why it prefers 10 over r=3 for 10 we gain 9.606 (10 * 0.99^4) and for 3 we gain 2.904 (10 * 0.99^2)\n",
        "\n",
        "### **Note:**\n",
        "It is worth noting that for each run, the policy iteration produces the same optimal policy as the value iteration, with the policy iteration algorithm being an order of magnitude faster (10x).\n",
        "\n"
      ],
      "metadata": {
        "id": "EQwXAylejC5p"
      }
    }
  ]
}